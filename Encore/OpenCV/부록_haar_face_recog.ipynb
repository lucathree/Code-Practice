{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-contrib-python --user\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def face_recog_train():\n",
    "    dataset_path = 'dataset/'\n",
    "    dirs = os.listdir(dataset_path)#dataset 하위 디렉토리 이름을 리스트에 저장\n",
    "    persons = []\n",
    "    for dir in dirs: #각 디렉토리에 저장된 파일명들을 persons에 담음\n",
    "        if os.path.isdir(dataset_path+dir):\n",
    "            files = os.listdir(dataset_path+dir)\n",
    "            for idx, f in enumerate(files):\n",
    "                files[idx] = dataset_path+dir+'/'+f\n",
    "            persons.append(files)\n",
    "    print(persons)\n",
    "    \n",
    "    classifiers = cv2.CascadeClassifier('haar/haarcascade_frontalface_alt2.xml')#얼굴 분류기\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create() # 얼굴 인식기 객체\n",
    "    samples = []  # 학습 데이터\n",
    "    ids = []  # 레이블(정답)\n",
    "    #학습할 얼굴 샘플링\n",
    "    for id, row in enumerate(persons):  #id:0, row:[정우성 사진들] / id:1, row:[이나영 사진들]\n",
    "        for p in row:\n",
    "            img = cv2.imread(p)\n",
    "            print(p)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            face = classifiers.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.2,\n",
    "                minNeighbors=5,\n",
    "                minSize=(20, 20)\n",
    "            )\n",
    "            for (x,y,w,h) in face:\n",
    "                samples.append(gray[y:y+h, x:x+w])\n",
    "                ids.append(id)\n",
    "\n",
    "    recognizer.train(samples, np.array(ids))\n",
    "    recognizer.write('trainer.yml')\n",
    "    print('얼굴 학습 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dataset/p1/AnyConv.com__1.jpg', 'dataset/p1/AnyConv.com__10.jpg', 'dataset/p1/AnyConv.com__2.jpg', 'dataset/p1/AnyConv.com__3.jpg', 'dataset/p1/AnyConv.com__4.jpg', 'dataset/p1/AnyConv.com__6.jpg', 'dataset/p1/AnyConv.com__7.jpg', 'dataset/p1/AnyConv.com__8.jpg', 'dataset/p1/AnyConv.com__9.jpg'], ['dataset/p2/AnyConv.com__1.jpg', 'dataset/p2/AnyConv.com__10.jpg', 'dataset/p2/AnyConv.com__2.jpg', 'dataset/p2/AnyConv.com__3.jpg', 'dataset/p2/AnyConv.com__4.jpg', 'dataset/p2/AnyConv.com__5.jpg', 'dataset/p2/AnyConv.com__6.jpg', 'dataset/p2/AnyConv.com__7.jpg', 'dataset/p2/AnyConv.com__8.jpg', 'dataset/p2/AnyConv.com__9.jpg'], ['dataset/p3/AnyConv.com_ (1).jpeg', 'dataset/p3/AnyConv.com_ (1).jpg', 'dataset/p3/AnyConv.com_ (2).jpg', 'dataset/p3/AnyConv.com_ (3).jpg', 'dataset/p3/AnyConv.com_ (4).jpg', 'dataset/p3/AnyConv.com_ (5).jpg', 'dataset/p3/AnyConv.com_ (6).jpg', 'dataset/p3/AnyConv.com_ (7).jpg', 'dataset/p3/AnyConv.com_ (8).jpg']]\n",
      "dataset/p1/AnyConv.com__1.jpg\n",
      "dataset/p1/AnyConv.com__10.jpg\n",
      "dataset/p1/AnyConv.com__2.jpg\n",
      "dataset/p1/AnyConv.com__3.jpg\n",
      "dataset/p1/AnyConv.com__4.jpg\n",
      "dataset/p1/AnyConv.com__6.jpg\n",
      "dataset/p1/AnyConv.com__7.jpg\n",
      "dataset/p1/AnyConv.com__8.jpg\n",
      "dataset/p1/AnyConv.com__9.jpg\n",
      "dataset/p2/AnyConv.com__1.jpg\n",
      "dataset/p2/AnyConv.com__10.jpg\n",
      "dataset/p2/AnyConv.com__2.jpg\n",
      "dataset/p2/AnyConv.com__3.jpg\n",
      "dataset/p2/AnyConv.com__4.jpg\n",
      "dataset/p2/AnyConv.com__5.jpg\n",
      "dataset/p2/AnyConv.com__6.jpg\n",
      "dataset/p2/AnyConv.com__7.jpg\n",
      "dataset/p2/AnyConv.com__8.jpg\n",
      "dataset/p2/AnyConv.com__9.jpg\n",
      "dataset/p3/AnyConv.com_ (1).jpeg\n",
      "dataset/p3/AnyConv.com_ (1).jpg\n",
      "dataset/p3/AnyConv.com_ (2).jpg\n",
      "dataset/p3/AnyConv.com_ (3).jpg\n",
      "dataset/p3/AnyConv.com_ (4).jpg\n",
      "dataset/p3/AnyConv.com_ (5).jpg\n",
      "dataset/p3/AnyConv.com_ (6).jpg\n",
      "dataset/p3/AnyConv.com_ (7).jpg\n",
      "dataset/p3/AnyConv.com_ (8).jpg\n",
      "얼굴 학습 완료\n"
     ]
    }
   ],
   "source": [
    "face_recog_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_recog():\n",
    "\n",
    "    classifiers = cv2.CascadeClassifier('haar/haarcascade_frontalface_alt2.xml')\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "    \n",
    "    recognizer.read('trainer.yml')\n",
    "\n",
    "    names=['정우성', '이나영', '고창석']\n",
    "    img = cv2.imread('dataset/test1.jpg')\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face = classifiers.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.2,\n",
    "        minNeighbors=5,\n",
    "        minSize=(20, 20)\n",
    "    )\n",
    "    res = False\n",
    "    name = 'no face'\n",
    "    for (x, y, w, h) in face:\n",
    "        print('얼굴감지')\n",
    "        id, confi = recognizer.predict(gray[y:y+h, x:x+w])\n",
    "        #if confi < 50:\n",
    "        name = names[id]\n",
    "        print(name,'/ confidence:', confi)\n",
    "        res = True\n",
    "        \n",
    "    if len(face)==0:\n",
    "        name = 'unknown'\n",
    "        res = False\n",
    "\n",
    "    return res, name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 'unknown')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_recog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "c = cv2.CascadeClassifier('haar/strawberrydetector.xml')\n",
    "img = cv2.imread('img/s2.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "straw = c.detectMultiScale(gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(40, 40))\n",
    "\n",
    "img2 = img.copy()\n",
    "for (x, y, w, h) in straw:\n",
    "    #print(x)\n",
    "    cv2.rectangle(img2, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "if len(straw)==0:\n",
    "    print('no strawberry')\n",
    "else:\n",
    "    cv2.imshow('img', img)\n",
    "    cv2.imshow('res', img2)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "탐지\n",
      "탐지\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pytesseract import *\n",
    "\n",
    "path = 'img/car/'\n",
    "files = os.listdir(path)\n",
    "src = []\n",
    "imgs = []\n",
    "\n",
    "for f in files:\n",
    "    img = cv2.imread(path+f)#이미지 로드\n",
    "    img = cv2.resize(img, (400, 300))   \n",
    "    src.append(img.copy())\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)#흑백처리\n",
    "    blur = cv2.GaussianBlur(gray, (3,3), 0)#노이즈 제거\n",
    "    _, dst = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)#쓰레숄딩\n",
    "    imgs.append(dst)\n",
    "    \n",
    "for idx, th in enumerate(imgs):\n",
    "    cont, _ = cv2.findContours(th, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)#외곽선 검출\n",
    "\n",
    "    for c in cont:\n",
    "        \n",
    "        if cv2.contourArea(c) < 10000 or cv2.contourArea(c) > 30000:  #  너무 작으면 무시\n",
    "            continue\n",
    "\n",
    "        approx = cv2.approxPolyDP(c, cv2.arcLength(c, True)*0.02, True)\n",
    "        vtc = len(approx)\n",
    "        \n",
    "        if vtc==4:\n",
    "            print('탐지')\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            cv2.rectangle(src[idx], (x, y, w, h), (0, 255, 255))\n",
    "            \n",
    "            roi = src[idx][y:y+h,x:x+w, :]\n",
    "            cv2.imshow('img_'+str(idx), roi)\n",
    "            #text = pytesseract.image_to_string(roi,lang='kor')\n",
    "            #print(text+\" 차량 입차\")\n",
    "        \n",
    "    #cv2.imshow('img'+str(idx), src[idx])\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍슽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원 전 지사는 이날\n",
      "후 6시까지 작성한 녹취록이 아닌 녹음파일 전체를 공개하라\"고 압박\n",
      "날 발 늦게 일부 녹취록을\n",
      "\n",
      "구한 것이다.\n",
      "\n",
      "날 오전 9시 여의도 당사에서 긴급 기자회견을 열고 \"이 대표는 오늘\n",
      "헝     | 다\n",
      "록을 공개하며 반격에 나서자 녹음파일 전체를 모두 공개하라고 요\n",
      "\n",
      "지\n",
      "록 일부를 공개했다\"면서 \"제 기역과 양\n",
      "\n",
      "발언의 대상은 윤석열 후보\"라고 주장했다.\n",
      "\n",
      "|사는 \"이 대표는 파문이 확산하고\n",
      "침\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import *\n",
    "\n",
    "filename = \"img/sampletxt2.jpg\"\n",
    "image = Image.open(filename)\n",
    "text = image_to_string(image, lang='kor')\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
